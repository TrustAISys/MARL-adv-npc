{
  "batch_mode": "truncate_episodes",
  "clip_param": 0.3,
  "entropy_coeff": 0.02,
  "entropy_coeff_schedule": [
    [
      0,
      0.02
    ],
    [
      2000000,
      0.01
    ],
    [
      4000000,
      0.005
    ],
    [
      6000000,
      0.001
    ],
    [
      8000000,
      1e-06
    ]
  ],
  "env": "macad_CriticalScenario",
  "evaluation_interval": 0,
  "framework": "torch",
  "kl_coeff": 0.2,
  "lambda": 0.99,
  "lr": 0.0005,
  "lr_schedule": [
    [
      0,
      0.0005
    ],
    [
      100000,
      0.0001
    ],
    [
      500000,
      5e-05
    ],
    [
      1000000,
      1e-05
    ],
    [
      10000000,
      1e-07
    ]
  ],
  "model": {
    "custom_model": "Centralized_Critic_Model",
    "custom_model_config": {
      "agent_level_batch_update": false,
      "agent_name_ls": [
        "car1",
        "car2",
        "walker1"
      ],
      "algorithm": "mappo",
      "checkpoint_end": true,
      "checkpoint_freq": 1,
      "checkpoint_score_attr": "episode_reward_mean",
      "env": "macad",
      "env_args": {
        "concat_action_space": false,
        "map_name": "CriticalScenario",
        "pad_action_space": false,
        "use_only_camera": false,
        "use_only_semantic": true
      },
      "episode_limit": 300,
      "evaluation_interval": 0,
      "force_coop": false,
      "framework": "torch",
      "global_state_flag": false,
      "keep_checkpoints_num": 10,
      "local_dir": "~/ray_results",
      "local_mode": false,
      "mask_flag": true,
      "max_failures": -1,
      "model_arch_args": {
        "core_arch": "gru",
        "fc_layer": 4,
        "hidden_state_size": 256,
        "out_dim_fc_0": 128,
        "out_dim_fc_1": 256,
        "out_dim_fc_2": 256,
        "out_dim_fc_3": 128
      },
      "num_agents": 3,
      "num_cpus_per_worker": 1,
      "num_gpus": 1,
      "num_gpus_per_worker": 0,
      "num_workers": 1,
      "opp_action_in_cc": false,
      "policy_mapping_info": {
        "all_scenario": {
          "all_agents_one_policy": true,
          "description": "macad all scenarios",
          "one_agent_one_policy": true,
          "team_prefix": [
            "agent_"
          ]
        }
      },
      "restore_path": {
        "model_path": "",
        "params_path": ""
      },
      "resume": false,
      "seed": 321,
      "share_policy": "group",
      "space_act": "Discrete(9)",
      "space_obs": "Dict(action_mask:Box([0. 0. 0. 0. 0. 0. 0. 0. 0.], [1. 1. 1. 1. 1. 1. 1. 1. 1.], (9,), float32), obs:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf inf], (48,), float32))",
      "stop_iters": 9999999,
      "stop_reward": 999999,
      "stop_timesteps": 90000000
    }
  },
  "multiagent": {
    "policies": "{'shared_policy'}",
    "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7d8b40135d30>"
  },
  "num_gpus": 1,
  "num_gpus_per_worker": 0,
  "num_sgd_iter": 10,
  "num_workers": 1,
  "seed": 321,
  "sgd_minibatch_size": 1500,
  "simple_optimizer": false,
  "train_batch_size": 1500,
  "use_gae": true,
  "vf_clip_param": 10.0,
  "vf_loss_coeff": 1.0
}